---
title: "Linear Regression"
date: 2024-04-23
tags: ["Machine Learning", "Deap Learning"]
---

## Overview

K-NN 알고리즘은 데이터 간의 distance를 측정하여 가까운 데이터들을 찾아 라벨을 예측하였는데요.

K-NN을 신뢰하기 위해선 distance가 신뢰할 수 있는 metric인지를 따져봐야만 합니다.

![alt text](/blog/linear_regression/image.png)

위의 이미지는 모두 같은 L2 distance를 가지는 이미지들인데요.

pixel에 대한 단순 distance metric이 이미지의 문맥적인 정보를 전혀 구별하지 못한다는 점을 시사합니다.

또한 크기가 $RGB \times H \times W$인 학습 샘플 데이터 $N$개와 K-NN 파라미터 $K$에 대해서 예측을 한다면

시간 복잡도는 $O(HWNK)$로 보통 작은 이미지 크기가 800 * 600임을 감안하면 시간복잡도가 우습게 커질 것임을 알 수 있습니다.

따라서 좀 더 효율적인 알고리즘이 필요한데요.

이번에 다룰 선형회귀는 좀 더 근본적으로 입력과 출력 사이의 어떤 관계를 찾아보고자 하는 시도입니다.

## Model Parameters vs. Hyper Parameter

이제 Model Parameter란 개념이 등장하는데요.

Hyper Parameter의 경우 사전에 미리 정해진 user-defined 값을 의미한다면,

Model Parameter는 모델의 알고리즘에 의해 결정되는 값을 의미합니다.
따라서 Model Parameter는 모델에 의해 업데이트될 수 있고 이것은 Learnable하다고 표현합니다.

## Linear Regression with Linear algebra

Linear Regression은 Supervised Learning 알고리즘의 일종입니다.

Linear Regression은 주어진 입력 $x$와 출력 $y$ 사이의 선형적인 대응 관계를 찾는 것을 목표로 하는데요.

![alt text](/blog/linear_regression/image-1.png)

위 그림은 1차원 선형 관계를 통해 데이터의 분포를 학습한 결과를 나타냅니다.

데이터의 분포와 가장 잘 들어맞는 "선"을 찾아낸 것인데요.

좀 더 정확히 선형적인 대응 관계는
Linear map, Linear transform이라고 부르며 **"선형성(Linearity)"**을 만족하는 함수(function), 사상(map), 변환(transform)을 의미합니다.

함수, 사상, 변환은 모두 같은 것을 지칭하나 과목에 따라서 이름만 조금 다른 것입니다.

$$
\begin{align}
\begin{split}
&L: V \rightarrow W \text{ is said to be a Linear map if the following conditions are satisfied:}\\
&\forall v, u \in V, \forall c \in K \in \{\R, \mathbb{C} \},\\
&(1). \quad L(u + v) = L(u) + L(v) \quad\\
&(2). \quad L(cv) = cL(v)
\end{split}
\end{align}
$$

이 선형 사상을 다른 말로 준동형(homomorphism)이라고도 부르는데요.
만약 선형사상이 Invertible(역사상이 존재)하다면 동형(isomorphism)이라고 부르게 됩니다.
여기서 동형(isomorphism)은 사상의 양쪽 대상을 사실상 같은 것으로 볼 수 있다는 걸 의미합니다.

이제 벡터스페이스 간의 준동형이 의미하는 바를 좀 더 살펴봅시다.

$$
\begin{align}
\begin{split}
&\text{If V, W are finite dimensional vector spaces, then}\\
&\exists f: L(V, W) \rightarrow M_{n \times n} \\ 
&\text{where f is an isomorphism}
\end{split}
\end{align}
$$

따라서 어떤 텐서는 곧 Linear Map 그 자체로 볼 수가 있습니다.

Linear Map은 기본적으로 원점을 보존하는데요.
이 원점을 보존하는 성질이 제거된 녀석이 Linear Regression이 찾고자 하는 대응 관계가 됩니다.

따라서 Linear Regression은 사실 $Y=WX + b$가 되는 텐서 $W$와 $b$를 찾고자 하는 방법을 의미하고,
이때 $W$와 $b$가 바로 Model Parameter가 됩니다.

![alt text](/blog/linear_regression/image-2.png)

즉, Linear Regression의 학습은 모델이 적절한 가중치 $W, b$를 찾기 위한 알고리즘이고,

테스트는 최적의 가중치 $W, b$를 통해 주어진 데이터 분포를 반영하는 예측값 $\hat{Y}=WX + b$를 찾는 과정이 됩니다.

![alt text](/blog/linear_regression/image-3.png)

이제 예시를 살펴볼까요?

![alt text](/blog/linear_regression/image-4.png)

만약 건물의 온도(출력)를 예측하고 싶다면 영향을 끼칠 수 있는 요인(입력)을 몇 개 꼽을 수 있습니다.

위 그림에 따르면 입력은 총 4개, 출력은 1개인 형태가 될 것 입니다.

Linear Regression은 각 입력이 출력에 끼치는 영향을 가중치를 통해 선형의 형태로 파악합니다.

![alt text](/blog/linear_regression/image-5.png)

## Optimization

이제 최적의 가중치를 어떻게 찾을 수 있는지 알아볼 겁니다.

모델 파라미터는 예측값 $\hat{y}$를 결정하는데요.

만약 입력값 $x$에 대한 실제 출력 $y$와 $\hat{y}$가 있다면,

$y$와 $\hat{y}$가 비슷할 수록 모델이 잘 학습된 것이고, 차이가 클 수록 학습이 부족함을 의미합니다.

따라서 예측값과 실제값 사이의 오차를 나타낼 수 있는 적절한 손실 함수
$L(y, \hat{y})$를 정의할 수 있는데요. 이 때, 예측값은 가중치 $W, b$에 의해서 결정되기 때문에 손실함수는 $L(y, \hat{y}) = L(W, b)$로 나타낼 수 있습니다.

이제 모델의 학습은 바로 손실함수 $L(W)$를 최소화하는 과정으로 생각할 수 있습니다.
따라서 손실함수(Loss function)는 목적함수(Objective function), 비용함수(Cost function) 등으로 표현됩니다.

Linear Regression에서 손실함수는 어떻게 정의할 수 있을까요?

만약 부적절하게 정의할 경우 손실함수의 의미가 학습률을 나타내지 못할 수도 있게 됩니다.

Linear Regression의 경우 주로 아래와 같이 손실함수를 정의합니다.

$$
Loss(W, b) = \frac{1}{n} \sum_1^n (y_i - \hat{y_i})^2 
$$

그러면 Linear Regression은 다음의 문제로 바라볼 수 있는데요.

$$
\begin{split}
\min_W{Loss(W)} &= \frac{1}{n} \sum_1^n (y_i - \hat{y_i})^2\\
&= (WX - y)^T(WX - y)
\end{split}
$$

이 때 이 문제는 단순하여 closed form solution이 사실 학습이 없어도 유일하게 존재함을 보장받을 수 있고 해를 쉽게 찾을 수 있습니다.

![alt text](/blog/linear_regression/image-6.png)

## Regression vs Classification

둘의 차이는 연속적인 출력값을 얻을 것인지, 아니면 불연속적인 출력값을 얻을 것인지로 생각할 수 있습니다.

만약 Linear Regression의 방법을 classification에 적용하고 싶다면 어떻게 해야할까요?

Binary Classification은 출력으로 0 혹은 1을 기대하는 분류인데요.

Sigmoid라는 함수는 아래와 같이 정의되는 함수입니다.

$ \sigma(x) = \frac{1}{1 + e^{-x}}$

이 함수의 특징은 출력이 반드시 $(0, 1)$로 나온다는 점인데요.

![alt text](/blog/linear_regression/image-7.png)

따라서 sigmoid를 조건부 확률 $p(y_i=1|x_i, W)$로 취급할 수 있습니다.

따라서 $y_i=1$일 때 확률이 높을 수록 좋은 성능을 가진 모델이라 평가할 수 있는데요.

이 때 손실함수는 아래와 같이 정의해볼 수 있습니다.

우선 i번째 샘플 $x_i$에 대한 출력과 실제값 $\hat{y_i}, y_i$에 대한 손실함수는 아래와 같이 정의합니다.

$\mathcal{L}(\hat{y_i}, y_i) = y_i\log{\hat{y_i}} + (1-y_i)\log({1-\hat{y_i}})$

이 함수를 해석해보면, 라벨 $y_i$는 0 또는 1이기 때문에 두 항 중 하나만 반영될 수 있음을 의미합니다.

또한 $\hat{y_i} = p(y_i=1|x_i, W)$이므로 $1 - \hat{y_i} = p(y_i=0|x_i, W)$가 됩니다.

log가 붙는 이유는 확률은 $[0, 1]$에서 나타나기 떄문에 이에 대해서 좀 더 스케일을 늘리기 위함입니다.

이제 전체 샘플에 대해서는 이를 평균내어 비용함수를 구하게 됩니다.

$C(W) = - \frac{1}{n}\sum_1^n \mathcal{L}(\hat{y_i}, y_i)$

여기서 -가 붙는 이유는 log의 $(0, 1)$ 값이 음수가 되기 때문에 계산의 편의성을 위함입니다.

